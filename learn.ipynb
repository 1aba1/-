{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff223689",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-06-11T17:48:51.114035400Z",
     "start_time": "2024-06-11T17:48:45.460655400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/217 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b8d659577c64b5db42f8894725937b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\f'l\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/577 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23d1a176483c442d9366dbaf05a16741"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/110k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27d860f087294cbbaa22da6e6d2de784"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a51f1fa0d6f148a58e433e51b969cd91"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='uer/gpt2-chinese-cluecorpussmall', vocab_size=21128, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'input_ids': [[101, 3617, 1139, 3313, 1139, 1045, 6793, 6809, 117, 1283, 2255, 674, 2255, 1963, 4125, 1355, 119, 7557, 5640, 6624, 1403, 1921, 677, 3341, 117, 6852, 1316, 3655, 3215, 6628, 1316, 3299, 119, 102], [101, 4007, 4680, 3736, 2255, 1724, 3307, 2406, 117, 4635, 756, 7770, 1318, 2322, 4170, 3119, 119, 3189, 1726, 4896, 2512, 4959, 4541, 3312, 117, 7599, 6853, 4351, 1898, 1057, 2207, 3517, 119, 6823, 2273, 849, 2242, 3566, 4819, 5862, 117, 3171, 2359, 1963, 1383, 2779, 704, 3837, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#加载编码器：将文本编码成数字，让系统可以运算\n",
    "# 预训练的模型是白话文，而非文言文，后续工作是在这个模型上进行微调\n",
    "tokenizer = AutoTokenizer.from_pretrained('uer/gpt2-chinese-cluecorpussmall')\n",
    "\n",
    "print(tokenizer)\n",
    "\n",
    "#编码试算\n",
    "tokenizer.batch_encode_plus([\n",
    "    '欲出未出光辣达,千山万山如火发.须臾走向天上来,逐却残星赶却月.',\n",
    "    '满目江山四望幽,白云高卷嶂烟收.日回禽影穿疏木,风递猿声入小楼.远岫似屏横碧落,断帆如叶截中流.'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc8dfdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T17:51:43.323133600Z",
     "start_time": "2024-06-11T17:51:43.002741500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(304752, '欲出未出光辣达,千山万山如火发.须臾走向天上来,逐却残星赶却月.')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "#简单数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        # 指定文件编码为 'utf-8'\n",
    "        # 50万行的诗 五言绝句和七言绝句 让模型模拟出诗歌的写作风格\n",
    "        with open('chinese_poems.txt', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        lines = [i.strip() for i in lines]\n",
    "\n",
    "        self.lines = lines\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.lines[i]\n",
    "\n",
    "\n",
    "dataset = Dataset()\n",
    "\n",
    "len(dataset), dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "750a592f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T17:51:57.761098300Z",
     "start_time": "2024-06-11T17:51:50.145364500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(839587, '云髻高梳鬓不分，扫除虚室事元君。新糊白纸屏风上，尽画蓬莱五色云。')"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#更多数据数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        data = []\n",
    "        for i in os.listdir('more_datas'):\n",
    "            if i == '.ipynb_checkpoints':\n",
    "                continue\n",
    "            data.append(pd.read_csv('more_datas/%s' % i))\n",
    "\n",
    "        data = pd.concat(data).reset_index()\n",
    "\n",
    "        data = data['内容']\n",
    "\n",
    "        data = data.str.strip()\n",
    "        # 强制格式\n",
    "        #移除一些标点符号\n",
    "        data = data.str.replace('[《》“”「」]', '', regex=True)\n",
    "\n",
    "        #正则过滤\n",
    "        select = data.str.match('^[\\w，。？、！：；]+$', na=False)\n",
    "        data = data[select]\n",
    "\n",
    "        #标点符号合并\n",
    "        data = data.str.replace('[？！；]', '。', regex=True)\n",
    "        data = data.str.replace('[、：]', '，', regex=True)\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data.iloc[i]\n",
    "\n",
    "\n",
    "dataset = Dataset()\n",
    "\n",
    "len(dataset), dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "405095b6",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-06-11T17:52:38.700346700Z",
     "start_time": "2024-06-11T17:52:38.599976300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([8, 512])\n",
      "token_type_ids torch.Size([8, 512])\n",
      "attention_mask torch.Size([8, 512])\n",
      "labels torch.Size([8, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": "104948"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#数据整理\n",
    "def collate_fn(data):\n",
    "    #古诗编码成数字\n",
    "    data = tokenizer.batch_encode_plus(data,\n",
    "                                       padding=True,\n",
    "                                       truncation=True,\n",
    "                                       max_length=512,\n",
    "                                       return_tensors='pt')\n",
    "    #编码结果增加一个“labels”键\n",
    "    data['labels'] = data['input_ids'].clone()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=8,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "for i, data in enumerate(loader):\n",
    "    break\n",
    "\n",
    "for k, v in data.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f53f3c45",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-06-11T17:58:15.310265200Z",
     "start_time": "2024-06-11T17:52:59.225680900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/421M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e35f95707ad34760a97d30723afb5ede"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10206.8736\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor(11.0360), torch.Size([8, 512, 21128]))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, GPT2Model\n",
    "\n",
    "#加载模型 checkpoint\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'uer/gpt2-chinese-cluecorpussmall')\n",
    "\n",
    "# 定义模型：直接使用预训练的模型\n",
    "\n",
    "#统计参数量\n",
    "print(sum(i.numel() for i in model.parameters()) / 10000)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(**data)\n",
    "\n",
    "out['loss'], out['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbd9b9ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T17:59:09.075523900Z",
     "start_time": "2024-06-11T17:58:23.905467700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS] 秋 高 气 爽 的 ， 有 名 的 水 手 。 在 这 里 见 多 ， 觉 得 好 吃 的 。\n",
      "1 [CLS] 秋 高 气 爽 感 ， 赏 月 赏 月 赏 。 每 人 皆 有 限 ， 有 限 的 限 制 。\n",
      "2 [CLS] 秋 高 气 爽 不 ， 秋 冬 天 气 爽 。 看 了 这 么 多 ， 突 然 想 起 这 。\n"
     ]
    }
   ],
   "source": [
    "def generate(text, row, col):\n",
    "\n",
    "    def generate_loop(data):\n",
    "        with torch.no_grad():\n",
    "            out = model(**data)\n",
    "\n",
    "        #取最后一个字\n",
    "        #[5, b, 50257]\n",
    "        out = out['logits']\n",
    "        #[5, 50257] 取最后一个字\n",
    "        out = out[:, -1]\n",
    "\n",
    "        #第50大的值,以此为分界线,小于该值的全部赋值为负无穷\n",
    "        #[5, 50257] -> [5, 50]\n",
    "        topk_value = torch.topk(out, 50).values\n",
    "        #[5, 50] -> [5] -> [5, 1]\n",
    "        topk_value = topk_value[:, -1].unsqueeze(dim=1)\n",
    "\n",
    "        #赋值\n",
    "        #[5, 50257]\n",
    "        out = out.masked_fill(out < topk_value, -float('inf'))\n",
    "\n",
    "        #不允许写特殊符号\n",
    "        out[:, tokenizer.sep_token_id] = -float('inf')\n",
    "        out[:, tokenizer.unk_token_id] = -float('inf')\n",
    "        out[:, tokenizer.pad_token_id] = -float('inf')\n",
    "        for i in '，。':\n",
    "            out[:, tokenizer.get_vocab()[i]] = -float('inf')\n",
    "\n",
    "        #根据概率采样,无放回,所以不可能重复\n",
    "        #[5, 50257] -> [5, 1]\n",
    "        out = out.softmax(dim=1)\n",
    "        out = out.multinomial(num_samples=1)\n",
    "\n",
    "        #强制添加标点符号\n",
    "        c = data['input_ids'].shape[1] / (col + 1)\n",
    "        if c % 1 == 0:\n",
    "            if c % 2 == 0:\n",
    "                out[:, 0] = tokenizer.get_vocab()['。']\n",
    "            else:\n",
    "                out[:, 0] = tokenizer.get_vocab()['，']\n",
    "\n",
    "        data['input_ids'] = torch.cat([data['input_ids'], out], dim=1)\n",
    "        data['attention_mask'] = torch.ones_like(data['input_ids'])\n",
    "        data['token_type_ids'] = torch.zeros_like(data['input_ids'])\n",
    "        data['labels'] = data['input_ids'].clone()\n",
    "\n",
    "        if data['input_ids'].shape[1] >= row * col + row + 1:\n",
    "            return data\n",
    "\n",
    "        return generate_loop(data)\n",
    "\n",
    "    #重复3遍 生成3句话\n",
    "    # 编码\n",
    "    data = tokenizer.batch_encode_plus([text] * 3, return_tensors='pt')\n",
    "    data['input_ids'] = data['input_ids'][:, :-1]\n",
    "    data['attention_mask'] = torch.ones_like(data['input_ids'])\n",
    "    data['token_type_ids'] = torch.zeros_like(data['input_ids'])\n",
    "    data['labels'] = data['input_ids'].clone()\n",
    "\n",
    "    data = generate_loop(data)\n",
    "\n",
    "    for i in range(3):\n",
    "        print(i, tokenizer.decode(data['input_ids'][i]))\n",
    "\n",
    "\n",
    "generate('秋高气爽', row=4, col=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb0e6ed9",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-06-11T18:22:25.090973900Z",
     "start_time": "2024-06-11T18:06:32.442362200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10.397042274475098 4.99995235735793e-05 0.13551401869158877\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 52\u001B[0m\n\u001B[0;32m     48\u001B[0m     model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     49\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(model, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msave.model\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 52\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[11], line 21\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m data\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m     20\u001B[0m     data[k] \u001B[38;5;241m=\u001B[39m data[k]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m---> 21\u001B[0m out \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdata)\n\u001B[0;32m     22\u001B[0m loss \u001B[38;5;241m=\u001B[39m out[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     24\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\anaconda\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1068\u001B[0m, in \u001B[0;36mGPT2LMHeadModel.forward\u001B[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1065\u001B[0m     torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mset_device(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer\u001B[38;5;241m.\u001B[39mfirst_device)\n\u001B[0;32m   1066\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m hidden_states\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m-> 1068\u001B[0m lm_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlm_head\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1070\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1071\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1072\u001B[0m     \u001B[38;5;66;03m# Shift so that tokens < n predict n\u001B[39;00m\n",
      "File \u001B[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_scheduler\n",
    "\n",
    "\n",
    "#tuning训练\n",
    "def train():\n",
    "    global model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    scheduler = get_scheduler(name='linear',\n",
    "                              num_warmup_steps=0,\n",
    "                              num_training_steps=len(loader),\n",
    "                              optimizer=optimizer)\n",
    "\n",
    "    model.train()\n",
    "    for i, data in enumerate(loader):\n",
    "        for k in data.keys():\n",
    "            data[k] = data[k].to(device)\n",
    "        out = model(**data)\n",
    "        loss = out['loss']\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            labels = data['labels'][:, 1:]\n",
    "            out = out['logits'].argmax(dim=2)[:, :-1]\n",
    "\n",
    "            select = labels != 0\n",
    "            labels = labels[select]\n",
    "            out = out[select]\n",
    "            del select\n",
    "\n",
    "            accuracy = (labels == out).sum().item() / labels.numel()\n",
    "\n",
    "            lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "\n",
    "            print(i, loss.item(), lr, accuracy)\n",
    "\n",
    "    model = model.to('cpu')\n",
    "    torch.save(model, 'save.model')\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c6a16e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:12:50.063961600Z",
     "start_time": "2024-06-13T18:12:49.454963900Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./save/save.model\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      3\u001B[0m generate(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m锄禾日当午\u001B[39m\u001B[38;5;124m'\u001B[39m, row\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, col\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "model = torch.load('./save/save.model')\n",
    "\n",
    "generate('锄禾日当午', row=4, col=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
